# pseudo orchestration: accept query and chunks 

def run_llm(query, chunks):
    # build prompt 
    context = "\n".join([c['text'] for c in chunks])
    prompt = f"Context: \n{context}\n\nQuestion: {query}\nAnswer:"
    
    # call LLM (pseudo code) or local llm
    # for prototype, we just return a dummy answer
    answer = f"(prototype) Answer based on {len(chunks)} chunks"
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    




























































